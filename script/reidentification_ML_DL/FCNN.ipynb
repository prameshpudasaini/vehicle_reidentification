{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e0d186-ff03-4157-9332-eca236a3ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 5391\n",
      "Test dataset size: 619\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train and test datasets\n",
    "df_train = pd.read_csv(\"final_match_pairs_train.txt\", sep = '\\t') # inferred match pairs\n",
    "df_test = pd.read_csv(\"final_match_pairs_ground_truth_additional.txt\", sep = '\\t') # ground-truth match pairs\n",
    "\n",
    "# retain test dataset for testing reidentification algorithm\n",
    "data_test = df_test.copy(deep = True)\n",
    "\n",
    "# filter rows with match = 1\n",
    "df_train = df_train[df_train.match == 1]\n",
    "df_test = df_test[df_test.match == 1]\n",
    "\n",
    "# drop redundant columns\n",
    "index_cols = ['file', 'adv', 'stop']\n",
    "df_train.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "df_test.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "\n",
    "# test dataset of candidate adv where travel time are to be predicted\n",
    "X_adv = data_test.drop(index_cols + ['match', 'travel_time'], axis = 1)\n",
    "\n",
    "# split training features & target into train and test sets\n",
    "random_state = 42\n",
    "X_train = df_train.drop('travel_time', axis = 1)\n",
    "y_train = df_train.travel_time\n",
    "X_test = df_test.drop('travel_time', axis = 1)\n",
    "y_test = df_test.travel_time\n",
    "\n",
    "print(f\"Train dataset size: {len(X_train)}\")\n",
    "print(f\"Test dataset size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed633791-37b3-41ca-856d-eec59ace7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 15:18:53.629698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 15:18:54.123749: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/catsit/miniconda3/envs/tf/lib/\n",
      "2024-12-13 15:18:54.123830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/catsit/miniconda3/envs/tf/lib/\n",
      "2024-12-13 15:18:54.123836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 15:18:54.910755: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 15:18:54.910756: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 15:18:55.371220: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/catsit/miniconda3/envs/tf/lib/\n",
      "2024-12-13 15:18:55.371283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/catsit/miniconda3/envs/tf/lib/\n",
      "2024-12-13 15:18:55.371301: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/catsit/miniconda3/envs/tf/lib/\n",
      "2024-12-13 15:18:55.371307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-12-13 15:18:55.371329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/catsit/miniconda3/envs/tf/lib/\n",
      "2024-12-13 15:18:55.371334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-12-13 15:18:55.907797: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2024-12-13 15:18:55.907841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-JRCHV9T): /proc/driver/nvidia/version does not exist\n",
      "2024-12-13 15:18:55.907851: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2024-12-13 15:18:55.907865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-JRCHV9T): /proc/driver/nvidia/version does not exist\n",
      "2024-12-13 15:18:55.908013: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-13 15:18:55.908015: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 524us/step\n",
      "34/34 [==============================] - 0s 521us/step\n",
      "34/34 [==============================] - 0s 530us/step\n",
      "34/34 [==============================] - 0s 521us/step\n",
      "34/34 [==============================] - 0s 531us/step\n",
      "34/34 [==============================] - 0s 533us/step\n",
      "34/34 [==============================] - 0s 520us/step\n",
      "34/34 [==============================] - 0s 543us/step\n",
      "34/34 [==============================] - 0s 541us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 525us/step\n",
      "34/34 [==============================] - 0s 531us/step\n",
      "34/34 [==============================] - 0s 548us/step\n",
      "34/34 [==============================] - 0s 559us/step\n",
      "34/34 [==============================] - 0s 547us/step\n",
      "34/34 [==============================] - 0s 561us/step\n",
      "34/34 [==============================] - 0s 556us/step\n",
      "34/34 [==============================] - 0s 547us/step\n",
      "34/34 [==============================] - 0s 543us/step\n",
      "34/34 [==============================] - 0s 546us/step\n",
      "34/34 [==============================] - 0s 566us/step\n",
      "34/34 [==============================] - 0s 593us/step\n",
      "34/34 [==============================] - 0s 532us/step\n",
      "34/34 [==============================] - 0s 519us/step\n",
      "34/34 [==============================] - 0s 546us/step\n",
      "34/34 [==============================] - 0s 530us/step\n",
      "34/34 [==============================] - 0s 528us/step\n",
      "34/34 [==============================] - 0s 533us/step\n",
      "34/34 [==============================] - 0s 577us/step\n",
      "34/34 [==============================] - 0s 542us/step\n",
      "34/34 [==============================] - 0s 532us/step\n",
      "34/34 [==============================] - 0s 530us/step\n",
      "34/34 [==============================] - 0s 545us/step\n",
      "34/34 [==============================] - 0s 532us/step\n",
      "34/34 [==============================] - 0s 524us/step\n",
      "34/34 [==============================] - 0s 535us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 538us/step\n",
      "34/34 [==============================] - 0s 557us/step\n",
      "34/34 [==============================] - 0s 560us/step\n",
      "34/34 [==============================] - 0s 523us/step\n",
      "34/34 [==============================] - 0s 575us/step\n",
      "34/34 [==============================] - 0s 547us/step\n",
      "34/34 [==============================] - 0s 583us/step\n",
      "34/34 [==============================] - 0s 565us/step\n",
      "34/34 [==============================] - 0s 531us/step\n",
      "34/34 [==============================] - 0s 523us/step\n",
      "34/34 [==============================] - 0s 525us/step\n",
      "34/34 [==============================] - 0s 564us/step\n",
      "34/34 [==============================] - 0s 535us/step\n",
      "34/34 [==============================] - 0s 558us/step\n",
      "34/34 [==============================] - 0s 569us/step\n",
      "34/34 [==============================] - 0s 534us/step\n",
      "34/34 [==============================] - 0s 521us/step\n",
      "34/34 [==============================] - 0s 530us/step\n",
      "34/34 [==============================] - 0s 560us/step\n",
      "34/34 [==============================] - 0s 536us/step\n",
      "34/34 [==============================] - 0s 580us/step\n",
      "34/34 [==============================] - 0s 551us/step\n",
      "34/34 [==============================] - 0s 597us/step\n",
      "34/34 [==============================] - 0s 564us/step\n",
      "34/34 [==============================] - 0s 555us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 539us/step\n",
      "34/34 [==============================] - 0s 524us/step\n",
      "34/34 [==============================] - 0s 571us/step\n",
      "34/34 [==============================] - 0s 536us/step\n",
      "34/34 [==============================] - 0s 541us/step\n",
      "34/34 [==============================] - 0s 589us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 549us/step\n",
      "34/34 [==============================] - 0s 543us/step\n",
      "34/34 [==============================] - 0s 523us/step\n",
      "34/34 [==============================] - 0s 554us/step\n",
      "34/34 [==============================] - 0s 535us/step\n",
      "34/34 [==============================] - 0s 548us/step\n",
      "34/34 [==============================] - 0s 560us/step\n",
      "34/34 [==============================] - 0s 550us/step\n",
      "34/34 [==============================] - 0s 559us/step\n",
      "34/34 [==============================] - 0s 543us/step\n",
      "34/34 [==============================] - 0s 541us/step\n",
      "34/34 [==============================] - 0s 548us/step\n",
      "34/34 [==============================] - 0s 517us/step\n",
      "34/34 [==============================] - 0s 543us/step\n",
      "34/34 [==============================] - 0s 581us/step\n",
      "34/34 [==============================] - 0s 538us/step\n",
      "34/34 [==============================] - 0s 577us/step\n",
      "34/34 [==============================] - 0s 550us/step\n",
      "34/34 [==============================] - 0s 566us/step\n",
      "34/34 [==============================] - 0s 540us/step\n",
      "34/34 [==============================] - 0s 526us/step\n",
      "34/34 [==============================] - 0s 533us/step\n",
      "34/34 [==============================] - 0s 552us/step\n",
      "34/34 [==============================] - 0s 530us/step\n",
      "34/34 [==============================] - 0s 539us/step\n",
      "34/34 [==============================] - 0s 539us/step\n",
      "34/34 [==============================] - 0s 548us/step\n",
      "34/34 [==============================] - 0s 541us/step\n",
      "34/34 [==============================] - 0s 547us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 582us/step\n",
      "34/34 [==============================] - 0s 571us/step\n",
      "34/34 [==============================] - 0s 567us/step\n",
      "34/34 [==============================] - 0s 537us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 550us/step\n",
      "34/34 [==============================] - 0s 550us/step\n",
      "34/34 [==============================] - 0s 569us/step\n",
      "34/34 [==============================] - 0s 570us/step\n",
      "34/34 [==============================] - 0s 530us/step\n",
      "34/34 [==============================] - 0s 561us/step\n",
      "34/34 [==============================] - 0s 551us/step\n",
      "34/34 [==============================] - 0s 543us/step\n",
      "34/34 [==============================] - 0s 542us/step\n",
      "34/34 [==============================] - 0s 524us/step\n",
      "34/34 [==============================] - 0s 544us/step\n",
      "34/34 [==============================] - 0s 545us/step\n",
      "34/34 [==============================] - 0s 574us/step\n",
      "34/34 [==============================] - 0s 540us/step\n",
      "34/34 [==============================] - 0s 549us/step\n",
      "34/34 [==============================] - 0s 529us/step\n",
      "34/34 [==============================] - 0s 539us/step\n",
      "34/34 [==============================] - 0s 566us/step\n",
      "34/34 [==============================] - 0s 526us/step\n",
      "34/34 [==============================] - 0s 537us/step\n",
      "34/34 [==============================] - 0s 539us/step\n",
      "34/34 [==============================] - 0s 534us/step\n",
      "34/34 [==============================] - 0s 562us/step\n",
      "34/34 [==============================] - 0s 552us/step\n",
      "34/34 [==============================] - 0s 533us/step\n",
      "34/34 [==============================] - 0s 520us/step\n",
      "34/34 [==============================] - 0s 534us/step\n",
      "34/34 [==============================] - 0s 533us/step\n",
      "34/34 [==============================] - 0s 546us/step\n",
      "34/34 [==============================] - 0s 558us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 15:21:46.371994: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: UNKNOWN ERROR (100)\n",
      "2024-12-13 15:21:46.372035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-JRCHV9T): /proc/driver/nvidia/version does not exist\n",
      "2024-12-13 15:21:46.372246: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'dropout': 0.1, 'learning_rate': 0.001, 'units': (64, 32)}\n",
      "Best cross-validation score (negative MSE): -0.7724215321160649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_adv = scaler.transform(X_adv)\n",
    "\n",
    "# Define a wrapper class for FCNN\n",
    "class FCNNWrapper:\n",
    "    def __init__(self, units=(64, 32), dropout=0.2, learning_rate=0.001):\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        for unit in self.units:\n",
    "            model.add(tf.keras.layers.Dense(unit, activation='relu'))\n",
    "            model.add(tf.keras.layers.Dropout(self.dropout))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            loss='mse',\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.build_model()\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            verbose=0,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"The model has not been fitted yet.\")\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return -mean_squared_error(y, y_pred)  # Negative MSE for GridSearchCV\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"units\": self.units,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "# Custom scorer for RMSE\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'units': [(64, 32), (128, 64), (64, 32, 16)],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=FCNNWrapper(),\n",
    "    param_grid=param_grid,\n",
    "    scoring=mse_scorer,\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (negative MSE):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f6cbca-f1b7-4ded-b291-f6757b62909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 541us/step\n",
      "Validation RMSE: 0.8789, Test RMSE: 0.9901\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on validation fold\n",
    "rmse_valid = np.sqrt(abs(grid_search.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "print(f\"Validation RMSE: {rmse_valid:.4f}, Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330e2c6d-f4ce-4723-88f9-edf88d035b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 529us/step\n"
     ]
    }
   ],
   "source": [
    "# make predictions on candidate adv for reidentifying algorithm\n",
    "y_adv_pred = best_model.predict(X_adv)\n",
    "\n",
    "# add predicted travel time to dataset with both 1 and 0 matches\n",
    "data_pred = data_test.copy(deep = True)\n",
    "data_pred['y_pred'] = y_adv_pred\n",
    "\n",
    "# save predicted travel time values\n",
    "data_pred.to_csv(\"predicted_travel_time/FCNN.txt\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701c2973-78df-4e4d-b4ad-850e0038544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reidentification algorithm for file:  20230327_0700_1400.txt\n",
      "Running reidentification algorithm for file:  20221206_0945_1200.txt\n",
      "Running reidentification algorithm for file:  20221214_0645_0715.txt\n",
      "Running reidentification algorithm for file:  20230327_1415_1900.txt\n",
      "Running reidentification algorithm for file:  20221206_0845_0915.txt\n",
      "Running reidentification algorithm for file:  20221214_0945_1015.txt\n",
      "\n",
      "Num of candidate pairs: 1040\n",
      "\n",
      "TP, FP, FN: 532, 31, 87\n",
      "Accuracy, Precision, Recall, F1: 0.8865, 0.9449, 0.8595, 0.9002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tt_thru_min, tt_thru_max = 2.5, 12 # min, max of through travel time to constrain search space\n",
    "\n",
    "# function to process candidate match pairs\n",
    "def reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file):\n",
    "    thru_match_initial = [] # store initial candidate match pairs of adv to stop-bar det\n",
    "    \n",
    "    for i in id_adv:\n",
    "        adv_time = adf[adf.ID == i].TimeStamp.values[0]\n",
    "        adv_lane = adf[adf.ID == i].Lane.values[0]\n",
    "\n",
    "        # stop-bar det IDs on the same lane to look for a match\n",
    "        id_stop_look = set(sdf[sdf.Lane == adv_lane].ID)\n",
    "\n",
    "        for j in id_stop_look:\n",
    "            stop_time = sdf[sdf.ID == j].TimeStamp.values[0]\n",
    "\n",
    "            if stop_time > adv_time: # look forward in timestamp\n",
    "                tt_adv_stop = (stop_time - adv_time) / np.timedelta64(1, 's') # paired travel time\n",
    "\n",
    "                if tt_thru_min <= tt_adv_stop <= tt_thru_max:\n",
    "                    # get predicted travel time for file and id_adv\n",
    "                    Xi = data_pred.copy(deep = True)\n",
    "                    Xi = Xi[(Xi.file == file[:-4]) & (Xi.adv == i)].reset_index(drop = True) # discard .txt\n",
    "                    \n",
    "                    tt_predict = Xi.loc[0, 'y_pred'] # predicted travel time\n",
    "                    tt_diff = round(abs(tt_adv_stop - tt_predict), 4) # abs diff between paired & predicted\n",
    "\n",
    "                    # store adv ID, stop ID, travel time diff\n",
    "                    thru_match_initial.append([i, j, tt_diff])\n",
    "\n",
    "    # dicts to store the lowest error for each adv, stop ID\n",
    "    seen_adv_id, seen_stop_id = {}, {}\n",
    "\n",
    "    # iterate through each candidate pair\n",
    "    for pair in thru_match_initial:\n",
    "        adv_id, stop_id, error = pair\n",
    "\n",
    "        # check if adv ID not seen or if error is lower than seen error for that adv ID\n",
    "        if (adv_id not in seen_adv_id) or (error < seen_adv_id[adv_id][1]):\n",
    "            seen_adv_id[adv_id] = list([stop_id, error])\n",
    "\n",
    "        # check if stop ID not seen or if error is lower than seen error for that stop ID\n",
    "        if (stop_id not in seen_stop_id) or (error < seen_stop_id[stop_id][1]):\n",
    "            seen_stop_id[stop_id] = list([adv_id, error])\n",
    "\n",
    "    # match pairs for adv with lowest error\n",
    "    df_adv = pd.DataFrame(seen_adv_id, index = ['adv', 'stop']).T.reset_index()\n",
    "    df_adv.columns = ['adv', 'stop', 'error']\n",
    "\n",
    "    # match pairs for stop with lowest error\n",
    "    df_stop = pd.DataFrame(seen_stop_id, index = ['stop', 'adv']).T.reset_index()\n",
    "    df_stop.columns = ['stop', 'adv', 'error']\n",
    "    \n",
    "    return {'df_adv': df_adv, 'df_stop': df_stop}\n",
    "\n",
    "file_path = \"data\"\n",
    "files = os.listdir(file_path)  # list of processed files to run through reidentifying algorithm\n",
    "\n",
    "df_result = [] # store reidentified match pairs from each file\n",
    "\n",
    "for file in files:\n",
    "    print(\"Running reidentification algorithm for file: \", file)\n",
    "    # read events-processed file with timestamp data\n",
    "    df = pd.read_csv(os.path.join(file_path, file), sep = '\\t')\n",
    "    df.TimeStamp = pd.to_datetime(df.TimeStamp, format = '%Y-%m-%d %H:%M:%S.%f').sort_values()\n",
    "    df.dropna(axis = 0, inplace = True) # drop rows with Nan\n",
    "\n",
    "    # data frames for adv and stop-bar det\n",
    "    adf = df[df.Det == 'adv']\n",
    "    sdf = df[df.Det == 'stop']\n",
    "    id_adv = list(sorted(adf.ID))\n",
    "\n",
    "    # process candidate match pairs to get datasets of adv and stop pairs\n",
    "    candidate_match_result = reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file)\n",
    "    df_adv = candidate_match_result['df_adv']\n",
    "    df_stop = candidate_match_result['df_stop']\n",
    "\n",
    "    # resulting common match pairs\n",
    "    df_match_pair = df_adv.merge(df_stop, on = ['adv', 'stop', 'error'])\n",
    "    df_match_pair['file'] = file[:-4]\n",
    "    df_result.append(df_match_pair)\n",
    "\n",
    "match_result = pd.concat(df_result)\n",
    "match_result.to_csv(\"reidentification_result/FCNN.txt\", sep = '\\t')\n",
    "\n",
    "# ground-truth match pairs for index cols\n",
    "match_ground = data_test.copy(deep = True)\n",
    "num_candidate_pairs = match_ground.shape[0]\n",
    "print(f\"\\nNum of candidate pairs: {num_candidate_pairs}\\n\")\n",
    "\n",
    "# filter ground-truth match pairs for match == 1 and select index cols\n",
    "match_ground = match_ground[match_ground.match == 1][index_cols]\n",
    "\n",
    "# get true positive (TP), false positive (FP), and false negative (FN) matches   \n",
    "match_TP = pd.merge(match_result, match_ground, on = index_cols)\n",
    "match_FP = match_result.merge(match_ground, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "match_FN = match_ground.merge(match_result, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "\n",
    "# num of TP, FP, FN\n",
    "TP, FP, FN = match_TP.shape[0], match_FP.shape[0], match_FN.shape[0]\n",
    "TN = num_candidate_pairs - TP - FP - FN\n",
    "\n",
    "# compute metrics\n",
    "accuracy = round((TP + TN) / (TP + FP + FN + TN), 4)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2*precision*recall / (precision + recall)\n",
    "\n",
    "print(f\"TP, FP, FN: {TP}, {FP}, {FN}\")\n",
    "print(f\"Accuracy, Precision, Recall, F1: {accuracy:.4f}, {precision:.4f}, {recall:.4f}, {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4289d-8c3f-47e2-ac16-743918988d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
