{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f334d5d7-7c56-476a-9177-1de90d65a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8cbb469c-70a3-4dc7-be60-3f8a66462a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4043\n",
      "Validation dataset size: 1348\n",
      "Test dataset size: 337\n"
     ]
    }
   ],
   "source": [
    "# train and test datasets\n",
    "df_train = pd.read_csv(\"final_match_pairs_train.txt\", sep = '\\t') # inferred match pairs\n",
    "df_test = pd.read_csv(\"final_match_pairs_ground_truth.txt\", sep = '\\t') # ground-truth match pairs\n",
    "\n",
    "# retain test dataset for testing reidentification algorithm\n",
    "data_test = df_test.copy(deep = True)\n",
    "\n",
    "# filter rows with match = 1\n",
    "df_train = df_train[df_train.match == 1]\n",
    "df_test = df_test[df_test.match == 1]\n",
    "\n",
    "# drop redundant columns\n",
    "index_cols = ['file', 'adv', 'stop']\n",
    "df_train.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "df_test.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "\n",
    "# training features and target variable\n",
    "X = df_train.drop('travel_time', axis = 1)\n",
    "y = df_train.travel_time\n",
    "\n",
    "# split training features & target into train, validation sets\n",
    "random_state = 42\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, random_state = random_state)\n",
    "\n",
    "# testing features and target variable\n",
    "X_test = df_test.drop('travel_time', axis = 1)\n",
    "y_test = df_test.travel_time\n",
    "\n",
    "print(f\"Train dataset size: {len(X_train)}\")\n",
    "print(f\"Validation dataset size: {len(X_valid)}\")\n",
    "print(f\"Test dataset size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "330e2c6d-f4ce-4723-88f9-edf88d035b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit & predict travel time and evaluate performance\n",
    "def modelFitPredict(model):\n",
    "    # fit model with hypertuned parameters on train dataset\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions on validation set and evaluate metrics\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "    mape_valid = mean_absolute_percentage_error(y_valid, y_pred_valid)\n",
    "    rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    # make predictions on test set and evaluate metrics\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    mape_test = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    # fit model with hypertuned parameters on train + validation datasets\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # make predictions on test set and evaluate metrics\n",
    "    y_pred_test_full = model.predict(X_test)\n",
    "    mape_test_full = mean_absolute_percentage_error(y_test, y_pred_test_full)\n",
    "    rmse_test_full = np.sqrt(mean_squared_error(y_test, y_pred_test_full))\n",
    "    \n",
    "    metrics_pred = {'mape_valid': mape_valid,\n",
    "                    'rmse_valid': rmse_valid,\n",
    "                    'mape_test': mape_test,\n",
    "                    'rmse_test': rmse_test,\n",
    "                    'mape_test_full': mape_test_full,\n",
    "                    'rmse_test_full': rmse_test_full}\n",
    "    \n",
    "    # make predictions on candidate adv for reidentifying algorithm\n",
    "    X_adv = data_test.drop(index_cols + ['match', 'travel_time'], axis = 1)\n",
    "    y_adv_pred = model.predict(X_adv)\n",
    "    \n",
    "    # add predicted travel time to dataset with both 1 and 0 matches\n",
    "    data_pred = data_test.copy(deep = True)\n",
    "    data_pred['y_pred'] = y_adv_pred\n",
    "    \n",
    "    return {'metrics_pred': metrics_pred, 'data_pred': data_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "701c2973-78df-4e4d-b4ad-850e0038544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_thru_min, tt_thru_max = 2.5, 12 # min, max of through travel time to constrain search space\n",
    "\n",
    "# function to process candidate match pairs\n",
    "def reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file):\n",
    "    thru_match_initial = [] # store initial candidate match pairs of adv to stop-bar det\n",
    "    \n",
    "    for i in id_adv:\n",
    "        adv_time = adf[adf.ID == i].TimeStamp.values[0]\n",
    "        adv_lane = adf[adf.ID == i].Lane.values[0]\n",
    "\n",
    "        # stop-bar det IDs on the same lane to look for a match\n",
    "        id_stop_look = set(sdf[sdf.Lane == adv_lane].ID)\n",
    "\n",
    "        for j in id_stop_look:\n",
    "            stop_time = sdf[sdf.ID == j].TimeStamp.values[0]\n",
    "\n",
    "            if stop_time > adv_time: # look forward in timestamp\n",
    "                tt_adv_stop = (stop_time - adv_time) / np.timedelta64(1, 's') # paired travel time\n",
    "\n",
    "                if tt_thru_min <= tt_adv_stop <= tt_thru_max:\n",
    "                    # get predicted travel time for file and id_adv\n",
    "                    Xi = data_pred.copy(deep = True)\n",
    "                    Xi = Xi[(Xi.file == file[:-4]) & (Xi.adv == i)].reset_index(drop = True) # discard .txt\n",
    "                    \n",
    "                    tt_predict = Xi.loc[0, 'y_pred'] # predicted travel time\n",
    "                    tt_diff = round(abs(tt_adv_stop - tt_predict), 4) # abs diff between paired & predicted\n",
    "\n",
    "                    # store adv ID, stop ID, travel time diff\n",
    "                    thru_match_initial.append([i, j, tt_diff])\n",
    "\n",
    "    # dicts to store the lowest error for each adv, stop ID\n",
    "    seen_adv_id, seen_stop_id = {}, {}\n",
    "\n",
    "    # iterate through each candidate pair\n",
    "    for pair in thru_match_initial:\n",
    "        adv_id, stop_id, error = pair\n",
    "\n",
    "        # check if adv ID not seen or if error is lower than seen error for that adv ID\n",
    "        if (adv_id not in seen_adv_id) or (error < seen_adv_id[adv_id][1]):\n",
    "            seen_adv_id[adv_id] = list([stop_id, error])\n",
    "\n",
    "        # check if stop ID not seen or if error is lower than seen error for that stop ID\n",
    "        if (stop_id not in seen_stop_id) or (error < seen_stop_id[stop_id][1]):\n",
    "            seen_stop_id[stop_id] = list([adv_id, error])\n",
    "\n",
    "    # match pairs for adv with lowest error\n",
    "    df_adv = pd.DataFrame(seen_adv_id, index = ['adv', 'stop']).T.reset_index()\n",
    "    df_adv.columns = ['adv', 'stop', 'error']\n",
    "\n",
    "    # match pairs for stop with lowest error\n",
    "    df_stop = pd.DataFrame(seen_stop_id, index = ['stop', 'adv']).T.reset_index()\n",
    "    df_stop.columns = ['stop', 'adv', 'error']\n",
    "    \n",
    "    return {'df_adv': df_adv, 'df_stop': df_stop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f9a7ef74-eab6-4925-8fb0-75cef1e273c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"processed\"\n",
    "files = os.listdir(file_path)  # list of processed files to run through reidentifying algorithm\n",
    "\n",
    "# function to process each file for reidentifying match pairs\n",
    "def processFiles(data_pred):\n",
    "    df_result = [] # store reidentified match pairs from each file\n",
    "    \n",
    "    for file in files:\n",
    "        # print(\"Running reidentification algorithm for file: \", file)\n",
    "        # read events-processed file with timestamp data\n",
    "        df = pd.read_csv(os.path.join(file_path, file), sep = '\\t')\n",
    "        df.TimeStamp = pd.to_datetime(df.TimeStamp, format = '%Y-%m-%d %H:%M:%S.%f').sort_values()\n",
    "        df.dropna(axis = 0, inplace = True) # drop rows with Nan\n",
    "        \n",
    "        # data frames for adv and stop-bar det\n",
    "        adf = df[df.Det == 'adv']\n",
    "        sdf = df[df.Det == 'stop']\n",
    "        id_adv = list(sorted(adf.ID))\n",
    "        \n",
    "        # process candidate match pairs to get datasets of adv and stop pairs\n",
    "        candidate_match_result = reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file)\n",
    "        df_adv = candidate_match_result['df_adv']\n",
    "        df_stop = candidate_match_result['df_stop']\n",
    "        \n",
    "        # resulting common match pairs\n",
    "        df_match_pair = df_adv.merge(df_stop, on = ['adv', 'stop', 'error'])\n",
    "        df_match_pair['file'] = file[:-4]\n",
    "        df_result.append(df_match_pair)\n",
    "        \n",
    "    match_result = pd.concat(df_result)\n",
    "    return match_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5aac522-325b-4889-980e-282fd0b53fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth match pairs for index cols\n",
    "match_ground = data_test.copy(deep = True)[data_test.match == 1][index_cols]\n",
    "num_ground_match_pairs = match_ground.shape[0]\n",
    "\n",
    "# function for reidentification algorithm\n",
    "def evaluateMatchMetrics(data_pred):\n",
    "    # get match result from processing files for reidentifying algorithm\n",
    "    match_result = processFiles(data_pred)\n",
    "    \n",
    "    # get true positive (TP), false positive (FP), and false negative (FN) matches   \n",
    "    match_TP = pd.merge(match_result, match_ground, on = index_cols)\n",
    "    match_FP = match_result.merge(match_ground, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "    match_FN = match_ground.merge(match_result, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "    \n",
    "    # num of TP, FP, FN\n",
    "    TP, FP, FN = match_TP.shape[0], match_FP.shape[0], match_FN.shape[0]\n",
    "    num_match = list([TP, FP, FN])\n",
    "    \n",
    "    # compute metrics\n",
    "    accuracy = TP / num_ground_match_pairs\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2*precision*recall / (precision + recall)\n",
    "    \n",
    "    return {'num_match': num_match, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e228d978-e474-4246-804c-a9a60f9e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid of hyperparameters for each model\n",
    "dt_param = {\n",
    "    'max_depth': [None, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'criterion': ['friedman_mse', 'absolute_error']\n",
    "}\n",
    "\n",
    "sv_param = {\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5],\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "\n",
    "rf_param = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "xgb_param = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5d0622fe-4b61-414c-87f6-11ea1a0fd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to produce combination of hyperparameters to test reidentification accuracy\n",
    "def hyperparameterCombination(model_param):\n",
    "    keys = model_param.keys()\n",
    "    values = model_param.values()\n",
    "    value_comb = list(itertools.product(*values)) # generate all possible combination of values\n",
    "    \n",
    "    comb_list = [] # store list of dictionaries\n",
    "    for comb in value_comb:\n",
    "        comb_dict = dict(zip(keys, comb))\n",
    "        comb_list.append(comb_dict)\n",
    "        \n",
    "    return comb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bae24fab-1558-430e-a311-a8def87d9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processModelMetrics(method, param_comb):\n",
    "    # select model and its parameters\n",
    "    if method == 'dt':\n",
    "        model = DecisionTreeRegressor(**param_comb, random_state = random_state)\n",
    "    elif method == 'sv':\n",
    "        model = SVR(**param_comb)\n",
    "    elif method == 'rf':\n",
    "        model = RandomForestRegressor(**param_comb, random_state = random_state)\n",
    "    elif method == 'xgb':\n",
    "        model = xgb.XGBRegressor(**param_comb, random_state = random_state)\n",
    "    \n",
    "    # evaluate model's performance for travel time prediction\n",
    "    prediction_result = modelFitPredict(model)\n",
    "    metrics_pred = prediction_result['metrics_pred']\n",
    "    data_pred = prediction_result['data_pred']\n",
    "    \n",
    "    # result from reidentification algorithm\n",
    "    metrics_match = evaluateMatchMetrics(data_pred)\n",
    "    \n",
    "    # compute error in prediction for test dataset with both 1 and 0 matches\n",
    "    data_pred['error'] = data_pred['travel_time'] - data_pred['y_pred'] # compute error in predicting travel time\n",
    "    data_pred = data_pred[index_cols + ['error']] # retain only file, adv, stop, error columns\n",
    "    \n",
    "    return {'metrics_match': metrics_match, 'metrics_pred': metrics_pred, 'data_pred': data_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "607c0045-e991-4651-898e-4cd3f6983062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processHyperparameterMetrics(method, model_param):\n",
    "    model_comb = hyperparameterCombination(model_param) # all combinations of hyperparameters\n",
    "    print(f\"Evaluating {len(model_comb)} combinations\")\n",
    "    \n",
    "    max_match_f1 = 0 # minimum value of f1 score to beat from reidentification\n",
    "    min_pred_mape = 1 # maximum value of MAPE to beat from travel time prediction\n",
    "\n",
    "    for comb in model_comb:\n",
    "        comb_result = processModelMetrics(method, comb) # process model metrics for prediction and reidentification\n",
    "        \n",
    "        # all output of combination\n",
    "        metrics_match = comb_result['metrics_match']\n",
    "        metrics_pred = comb_result['metrics_pred']\n",
    "        data_pred = comb_result['data_pred']\n",
    "        \n",
    "        # store pred & match metrics for each combination of hyperparameters\n",
    "        list_comb_result = [comb, metrics_match, metrics_pred]\n",
    "        all_comb_metrics_file = method + '_all_comb_metrics.json'\n",
    "        with open(all_comb_metrics_file, 'a') as file:\n",
    "            json.dump(list_comb_result, file)\n",
    "            file.write('\\n')\n",
    "        \n",
    "        current_match_f1 = metrics_match['f1']\n",
    "        \n",
    "        # best hyperparameter combination for highest f1 score\n",
    "        if current_match_f1 > max_match_f1:\n",
    "            max_match_f1 = current_match_f1 # update max f1\n",
    "            best_match_comb = comb # best hyperparameter combination for matching\n",
    "            best_match_metrics = metrics_match # best match metrics\n",
    "            best_match_pred_metrics = metrics_pred # prediction metrics for best match combination\n",
    "            best_match_data_pred = data_pred # prediction error for best match combination\n",
    "\n",
    "        current_pred_mape = metrics_pred['mape_test_full']\n",
    "        \n",
    "        # best hyperparameter combination for lowest mape in travel time prediction\n",
    "        if current_pred_mape < min_pred_mape:\n",
    "            min_pred_mape = current_pred_mape # update min mape\n",
    "            best_pred_comb = comb # best hyperparameter combination for prediction\n",
    "            best_pred_metrics = metrics_pred # best pred metrics\n",
    "            best_pred_match_metrics = metrics_match # matching metrics for best prediction combination\n",
    "            best_pred_data_pred = data_pred # prediction error for best prediction combination\n",
    "\n",
    "    best_comb_metrics_file = method + '_best_comb_metrics.json' # store dictionaries\n",
    "    with open(best_comb_metrics_file, 'a') as file:\n",
    "        json.dump(best_match_comb, file)\n",
    "        file.write('\\n')\n",
    "        json.dump(best_match_metrics, file)\n",
    "        file.write('\\n')\n",
    "        json.dump(best_match_pred_metrics, file)\n",
    "        file.write('\\n')\n",
    "        json.dump(best_pred_comb, file)\n",
    "        file.write('\\n')\n",
    "        json.dump(best_pred_metrics, file)\n",
    "        file.write('\\n')\n",
    "        json.dump(best_pred_match_metrics, file)\n",
    "        \n",
    "    best_match_data_pred.to_csv(method + '_best_match_data_pred.txt', sep = '\\t')\n",
    "    best_pred_data_pred.to_csv(method + '_best_pred_data_pred.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bcc58b5-9610-4962-a4e7-b5369ea58493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 270 combinations\n",
      "Evaluating 108 combinations\n",
      "Evaluating 360 combinations\n",
      "Evaluating 6480 combinations\n"
     ]
    }
   ],
   "source": [
    "# run reidentification framework for all models\n",
    "processHyperparameterMetrics('dt', dt_param)\n",
    "processHyperparameterMetrics('sv', sv_param)\n",
    "processHyperparameterMetrics('rf', rf_param)\n",
    "processHyperparameterMetrics('xgb', xgb_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca57a5-5d0d-4409-9a3b-49134bdf84de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
