{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a62cdd2-ccce-4fdb-b209-2006e66662d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 5391\n",
      "Test dataset size: 619\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"/Users/prameshpudasaini/Documents/vehicle_reidentification\")\n",
    "\n",
    "# train and test datasets\n",
    "df_train = pd.read_csv(\"data/final_match_pairs_train.txt\", sep = '\\t') # inferred match pairs\n",
    "df_test = pd.read_csv(\"data/final_match_pairs_ground_truth_additional.txt\", sep = '\\t') # ground-truth match pairs\n",
    "\n",
    "# retain test dataset for testing reidentification algorithm\n",
    "data_test = df_test.copy(deep = True)\n",
    "\n",
    "# filter rows with match = 1\n",
    "df_train = df_train[df_train.match == 1]\n",
    "df_test = df_test[df_test.match == 1]\n",
    "\n",
    "# drop redundant columns\n",
    "index_cols = ['file', 'adv', 'stop']\n",
    "df_train.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "df_test.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "\n",
    "# test dataset of candidate adv where travel time are to be predicted\n",
    "X_adv = data_test.drop(index_cols + ['match', 'travel_time'], axis = 1)\n",
    "\n",
    "# split training features & target into train and test sets\n",
    "random_state = 42\n",
    "X_train = df_train.drop('travel_time', axis = 1)\n",
    "y_train = df_train.travel_time\n",
    "X_test = df_test.drop('travel_time', axis = 1)\n",
    "y_test = df_test.travel_time\n",
    "\n",
    "print(f\"Train dataset size: {len(X_train)}\")\n",
    "print(f\"Test dataset size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dae54c0-2810-4d80-ae7b-12070b5ee878",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best parameters: {'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 200, 'reg_alpha': 0.5, 'reg_lambda': 0.1}\n",
      "Best cross-validation score (negative MSE): -0.7217324189597909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Custom scorer for GridSearchCV (mean squared error)\n",
    "def custom_mse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)\n",
    "\n",
    "mse_scorer = make_scorer(custom_mse, greater_is_better=False)\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "xgb_param = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 5, 7],\n",
    "    'min_child_weight': [3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_param,\n",
    "    scoring=mse_scorer,\n",
    "    cv=kf,\n",
    "    verbose=1,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "# Train the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score (negative MSE):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce8ac88-1bef-4752-9259-d1108c38d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.8495, Test RMSE: 0.9522\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on validation fold\n",
    "rmse_valid = np.sqrt(abs(grid_search.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "print(f\"Validation RMSE: {rmse_valid:.4f}, Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a95543-cdf1-44b8-bdbc-b7fb3d190fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on candidate adv for reidentifying algorithm\n",
    "y_adv_pred = best_model.predict(X_adv)\n",
    "\n",
    "# add predicted travel time to dataset with both 1 and 0 matches\n",
    "data_pred = data_test.copy(deep = True)\n",
    "data_pred['y_pred'] = y_adv_pred\n",
    "\n",
    "# save predicted travel time values\n",
    "data_pred.to_csv(\"ignore/predicted_travel_time/XGB.txt\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae18b86-97fb-48b7-abe3-1f332549561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reidentification algorithm for file:  20221206_0845_0915.txt\n",
      "Running reidentification algorithm for file:  20221214_0945_1015.txt\n",
      "Running reidentification algorithm for file:  20221206_0945_1200.txt\n",
      "Running reidentification algorithm for file:  20230327_0700_1400.txt\n",
      "Running reidentification algorithm for file:  20230327_1415_1900.txt\n",
      "Running reidentification algorithm for file:  20221214_0645_0715.txt\n",
      "\n",
      "Num of candidate pairs: 1040\n",
      "\n",
      "TP, FP, FN: 532, 32, 87\n",
      "Accuracy, Precision, Recall, F1: 0.8856, 0.9433, 0.8595, 0.8994\n"
     ]
    }
   ],
   "source": [
    "tt_thru_min, tt_thru_max = 2.5, 12 # min, max of through travel time to constrain search space\n",
    "\n",
    "# function to process candidate match pairs\n",
    "def reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file):\n",
    "    thru_match_initial = [] # store initial candidate match pairs of adv to stop-bar det\n",
    "    \n",
    "    for i in id_adv:\n",
    "        adv_time = adf[adf.ID == i].TimeStamp.values[0]\n",
    "        adv_lane = adf[adf.ID == i].Lane.values[0]\n",
    "\n",
    "        # stop-bar det IDs on the same lane to look for a match\n",
    "        id_stop_look = set(sdf[sdf.Lane == adv_lane].ID)\n",
    "\n",
    "        for j in id_stop_look:\n",
    "            stop_time = sdf[sdf.ID == j].TimeStamp.values[0]\n",
    "\n",
    "            if stop_time > adv_time: # look forward in timestamp\n",
    "                tt_adv_stop = (stop_time - adv_time) / np.timedelta64(1, 's') # paired travel time\n",
    "\n",
    "                if tt_thru_min <= tt_adv_stop <= tt_thru_max:\n",
    "                    # get predicted travel time for file and id_adv\n",
    "                    Xi = data_pred.copy(deep = True)\n",
    "                    Xi = Xi[(Xi.file == file[:-4]) & (Xi.adv == i)].reset_index(drop = True) # discard .txt\n",
    "                    \n",
    "                    tt_predict = Xi.loc[0, 'y_pred'] # predicted travel time\n",
    "                    tt_diff = round(abs(tt_adv_stop - tt_predict), 4) # abs diff between paired & predicted\n",
    "\n",
    "                    # store adv ID, stop ID, travel time diff\n",
    "                    thru_match_initial.append([i, j, tt_diff])\n",
    "\n",
    "    # dicts to store the lowest error for each adv, stop ID\n",
    "    seen_adv_id, seen_stop_id = {}, {}\n",
    "\n",
    "    # iterate through each candidate pair\n",
    "    for pair in thru_match_initial:\n",
    "        adv_id, stop_id, error = pair\n",
    "\n",
    "        # check if adv ID not seen or if error is lower than seen error for that adv ID\n",
    "        if (adv_id not in seen_adv_id) or (error < seen_adv_id[adv_id][1]):\n",
    "            seen_adv_id[adv_id] = list([stop_id, error])\n",
    "\n",
    "        # check if stop ID not seen or if error is lower than seen error for that stop ID\n",
    "        if (stop_id not in seen_stop_id) or (error < seen_stop_id[stop_id][1]):\n",
    "            seen_stop_id[stop_id] = list([adv_id, error])\n",
    "\n",
    "    # match pairs for adv with lowest error\n",
    "    df_adv = pd.DataFrame(seen_adv_id, index = ['adv', 'stop']).T.reset_index()\n",
    "    df_adv.columns = ['adv', 'stop', 'error']\n",
    "\n",
    "    # match pairs for stop with lowest error\n",
    "    df_stop = pd.DataFrame(seen_stop_id, index = ['stop', 'adv']).T.reset_index()\n",
    "    df_stop.columns = ['stop', 'adv', 'error']\n",
    "    \n",
    "    return {'df_adv': df_adv, 'df_stop': df_stop}\n",
    "\n",
    "file_path = \"ignore/data_reidentification_modeling\"\n",
    "files = os.listdir(file_path)  # list of processed files to run through reidentifying algorithm\n",
    "\n",
    "df_result = [] # store reidentified match pairs from each file\n",
    "\n",
    "for file in files:\n",
    "    print(\"Running reidentification algorithm for file: \", file)\n",
    "    # read events-processed file with timestamp data\n",
    "    df = pd.read_csv(os.path.join(file_path, file), sep = '\\t')\n",
    "    df.TimeStamp = pd.to_datetime(df.TimeStamp, format = '%Y-%m-%d %H:%M:%S.%f').sort_values()\n",
    "    df.dropna(axis = 0, inplace = True) # drop rows with Nan\n",
    "\n",
    "    # data frames for adv and stop-bar det\n",
    "    adf = df[df.Det == 'adv']\n",
    "    sdf = df[df.Det == 'stop']\n",
    "    id_adv = list(sorted(adf.ID))\n",
    "\n",
    "    # process candidate match pairs to get datasets of adv and stop pairs\n",
    "    candidate_match_result = reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file)\n",
    "    df_adv = candidate_match_result['df_adv']\n",
    "    df_stop = candidate_match_result['df_stop']\n",
    "\n",
    "    # resulting common match pairs\n",
    "    df_match_pair = df_adv.merge(df_stop, on = ['adv', 'stop', 'error'])\n",
    "    df_match_pair['file'] = file[:-4]\n",
    "    df_result.append(df_match_pair)\n",
    "\n",
    "match_result = pd.concat(df_result)\n",
    "match_result.to_csv(\"ignore/reidentification_result/XGB.txt\", sep = '\\t')\n",
    "\n",
    "# ground-truth match pairs for index cols\n",
    "match_ground = data_test.copy(deep = True)\n",
    "num_candidate_pairs = match_ground.shape[0]\n",
    "print(f\"\\nNum of candidate pairs: {num_candidate_pairs}\\n\")\n",
    "\n",
    "# filter ground-truth match pairs for match == 1 and select index cols\n",
    "match_ground = match_ground[match_ground.match == 1][index_cols]\n",
    "\n",
    "# get true positive (TP), false positive (FP), and false negative (FN) matches   \n",
    "match_TP = pd.merge(match_result, match_ground, on = index_cols)\n",
    "match_FP = match_result.merge(match_ground, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "match_FN = match_ground.merge(match_result, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "\n",
    "# num of TP, FP, FN\n",
    "TP, FP, FN = match_TP.shape[0], match_FP.shape[0], match_FN.shape[0]\n",
    "TN = num_candidate_pairs - TP - FP - FN\n",
    "\n",
    "# compute metrics\n",
    "accuracy = round((TP + TN) / (TP + FP + FN + TN), 4)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2*precision*recall / (precision + recall)\n",
    "\n",
    "print(f\"TP, FP, FN: {TP}, {FP}, {FN}\")\n",
    "print(f\"Accuracy, Precision, Recall, F1: {accuracy:.4f}, {precision:.4f}, {recall:.4f}, {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9957b-1c9b-4fdd-b2a3-31c95da42b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
