{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e0d186-ff03-4157-9332-eca236a3ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 5391\n",
      "Test dataset size: 619\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train and test datasets\n",
    "df_train = pd.read_csv(\"final_match_pairs_train.txt\", sep = '\\t') # inferred match pairs\n",
    "df_test = pd.read_csv(\"final_match_pairs_ground_truth_additional.txt\", sep = '\\t') # ground-truth match pairs\n",
    "\n",
    "# retain test dataset for testing reidentification algorithm\n",
    "data_test = df_test.copy(deep = True)\n",
    "\n",
    "# filter rows with match = 1\n",
    "df_train = df_train[df_train.match == 1]\n",
    "df_test = df_test[df_test.match == 1]\n",
    "\n",
    "# drop redundant columns\n",
    "index_cols = ['file', 'adv', 'stop']\n",
    "df_train.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "df_test.drop(index_cols + ['match'], axis = 1, inplace = True)\n",
    "\n",
    "# test dataset of candidate adv where travel time are to be predicted\n",
    "X_adv = data_test.drop(index_cols + ['match', 'travel_time'], axis = 1)\n",
    "\n",
    "# split training features & target into train and test sets\n",
    "random_state = 42\n",
    "X_train = df_train.drop('travel_time', axis = 1)\n",
    "y_train = df_train.travel_time\n",
    "X_test = df_test.drop('travel_time', axis = 1)\n",
    "y_test = df_test.travel_time\n",
    "\n",
    "print(f\"Train dataset size: {len(X_train)}\")\n",
    "print(f\"Test dataset size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1ed1cb-14df-4595-acb9-0ef1a708dfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 108/108 [4:11:29<00:00, 139.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'num_trees': 16, 'tree_depth': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout': 0.1}, Best Avg RMSE: 0.855632786461278, Best Loss: 0.7325745261767331\n",
      "Final Training Loss: 0.7059998827925801\n",
      "Test RMSE: 0.9495836936714142\n",
      "Best Hyperparameters: {'num_trees': 16, 'tree_depth': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout': 0.1}\n",
      "Best Avg RMSE on Validation Folds: 0.855632786461278\n",
      "Test RMSE: 0.9495836936714142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch Dataset for Tabular Data\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Differentiable Oblivious Decision Tree Layer\n",
    "class DifferentiableTree(nn.Module):\n",
    "    def __init__(self, input_dim, num_trees, tree_depth):\n",
    "        super(DifferentiableTree, self).__init__()\n",
    "        self.num_trees = num_trees\n",
    "        self.tree_depth = tree_depth\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Linear layers for tree splits\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, 2**tree_depth) for _ in range(num_trees)]\n",
    "        )\n",
    "\n",
    "        # Leaf values for each tree\n",
    "        self.leaf_values = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(2**tree_depth)) for _ in range(num_trees)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        tree_outputs = []\n",
    "        for i in range(self.num_trees):\n",
    "            logits = self.linear_layers[i](x)  # Compute tree splits\n",
    "            probs = torch.sigmoid(logits)  # Use sigmoid for split probabilities\n",
    "            leaves = torch.matmul(probs, self.leaf_values[i])  # Weighted sum over leaves\n",
    "            tree_outputs.append(leaves.unsqueeze(-1))  # Add dimension for stacking\n",
    "\n",
    "        # Stack along the last dimension and average across trees\n",
    "        return torch.cat(tree_outputs, dim=-1)  # Shape: (batch_size, num_trees)\n",
    "\n",
    "\n",
    "# NODE Model\n",
    "class NODEModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_trees=128, tree_depth=5, dropout=0.1):\n",
    "        super(NODEModel, self).__init__()\n",
    "        self.tree_layer = DifferentiableTree(input_dim, num_trees, tree_depth)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(num_trees, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tree_layer(x)  # Shape: (batch_size, num_trees)\n",
    "        x = self.dropout(x)\n",
    "        return self.output_layer(x).squeeze(-1)  # Final regression output\n",
    "\n",
    "\n",
    "# Training and Validation Functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# Perform K-Fold Cross-Validation with Hyperparameter Tuning\n",
    "def k_fold_cv_NODE(X, y, param_grid, folds=5, epochs=50, patience=10):\n",
    "    # Convert y to a NumPy array for indexing compatibility\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # K-Fold setup\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Track best hyperparameters and score\n",
    "    best_params = None\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_avg_rmse = float('inf')\n",
    "\n",
    "    # Iterate over hyperparameter grid\n",
    "    for params in tqdm(param_grid):\n",
    "        fold_losses = []\n",
    "        fold_rmses = []  # To store RMSE for each fold\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X_scaled):\n",
    "            # Split into train and validation sets\n",
    "            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]  # No KeyError now\n",
    "\n",
    "            # Create datasets and loaders\n",
    "            train_data = TabularDataset(X_train, y_train)\n",
    "            val_data = TabularDataset(X_val, y_val)\n",
    "            train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "            val_loader = DataLoader(val_data, batch_size=params['batch_size'])\n",
    "\n",
    "            # Initialize model, optimizer, and loss\n",
    "            model = NODEModel(\n",
    "                input_dim=X.shape[1],\n",
    "                num_trees=params['num_trees'],\n",
    "                tree_depth=params['tree_depth'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # Early stopping\n",
    "            best_fold_loss = float('inf')\n",
    "            no_improvement = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "                val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "                if val_loss < best_fold_loss:\n",
    "                    best_fold_loss = val_loss\n",
    "                    no_improvement = 0\n",
    "                else:\n",
    "                    no_improvement += 1\n",
    "\n",
    "                if no_improvement >= patience:\n",
    "                    break\n",
    "\n",
    "            # Store fold validation loss and RMSE\n",
    "            fold_losses.append(best_fold_loss)\n",
    "            fold_rmses.append(np.sqrt(best_fold_loss))  # RMSE = sqrt(MSE)\n",
    "\n",
    "        # Calculate mean loss and RMSE across folds\n",
    "        mean_loss = np.mean(fold_losses)\n",
    "        mean_rmse = np.mean(fold_rmses)\n",
    "\n",
    "        # Update best parameters and model if better performance is achieved\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            best_avg_rmse = mean_rmse\n",
    "            best_params = params\n",
    "            best_model = model  # Save the best model\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}, Best Avg RMSE: {best_avg_rmse}, Best Loss: {best_loss}\")\n",
    "    return best_model, scaler, best_params, best_avg_rmse\n",
    "\n",
    "# Train the Final Model\n",
    "def train_best_model(X, y, best_params, epochs=50, patience=10):\n",
    "    # Convert y to NumPy array for compatibility with PyTorch\n",
    "    y = np.array(y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    train_data = TabularDataset(X_scaled, y)\n",
    "    train_loader = DataLoader(train_data, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Initialize the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = NODEModel(\n",
    "        input_dim=X.shape[1],\n",
    "        num_trees=best_params['num_trees'],\n",
    "        tree_depth=best_params['tree_depth'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # No validation here since we're training on the full training set\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Final Training Loss: {best_loss}\")\n",
    "    return model, scaler\n",
    "\n",
    "# Evaluate on the Test Set\n",
    "def evaluate_model(model, scaler, X_test, y_test):\n",
    "    # Preprocess the test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Convert to tensor and move to device\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = X_test_tensor.to(device)\n",
    "        y_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "    # Calculate MSE and RMSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Test RMSE: {rmse}\")\n",
    "    return rmse\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'num_trees': nt,\n",
    "        'tree_depth': td,\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': bs,\n",
    "        'dropout': dp\n",
    "    }\n",
    "    for nt in [16, 32, 64]\n",
    "    for td in [2, 3, 5]\n",
    "    for lr in [0.01, 0.001]\n",
    "    for bs in [32, 64]\n",
    "    for dp in [0.0, 0.1, 0.2]\n",
    "]\n",
    "\n",
    "# Best Hyperparameters: {'num_trees': 32, 'tree_depth': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout': 0.1}\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    # Perform 5-Fold CV with Hyperparameter Tuning\n",
    "    best_model, scaler, best_params, best_avg_rmse = k_fold_cv_NODE(\n",
    "        X_train.values, y_train, param_grid, folds=5, epochs=50, patience=10\n",
    "    )\n",
    "\n",
    "    # Train the final model on the full training set\n",
    "    final_model, final_scaler = train_best_model(\n",
    "        X_train.values, y_train, best_params, epochs=50, patience=10\n",
    "    )\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    test_rmse = evaluate_model(final_model, final_scaler, X_test.values, y_test)\n",
    "\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "    print(f\"Best Avg RMSE on Validation Folds: {best_avg_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d3d09e-aab3-4c0d-9e13-03ea0d2268eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss: 0.7028120156575942\n",
      "Test RMSE: 0.9396563348775868\n",
      "Best Hyperparameters: {'num_trees': 16, 'tree_depth': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout': 0.1}\n",
      "Best Avg RMSE on Validation Folds: 0.855632786461278\n",
      "Test RMSE: 0.9396563348775868\n"
     ]
    }
   ],
   "source": [
    "# Train the final model on the full training set\n",
    "final_model, final_scaler = train_best_model(\n",
    "    X_train.values, y_train, best_params, epochs=50, patience=10\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_rmse = evaluate_model(final_model, final_scaler, X_test.values, y_test)\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Avg RMSE on Validation Folds: {best_avg_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330e2c6d-f4ce-4723-88f9-edf88d035b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catsit/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def predict_with_NODE(model, scaler, X_adv):\n",
    "    # Preprocess the data using the scaler\n",
    "    X_adv_scaled = scaler.transform(X_adv)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    X_adv_tensor = torch.tensor(X_adv_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Move tensor to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_adv_tensor = X_adv_tensor.to(device)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_adv_tensor).cpu().numpy()  # Convert predictions to NumPy array\n",
    "\n",
    "    # Return predictions as a Pandas Series\n",
    "    return pd.Series(predictions, index=X_adv.index, name=\"predictions\")\n",
    "\n",
    "# Predict on the advanced DataFrame X_adv\n",
    "predictions = predict_with_NODE(final_model, final_scaler, X_adv)\n",
    "\n",
    "# add predicted travel time to dataset with both 1 and 0 matches\n",
    "data_pred = data_test.copy(deep = True)\n",
    "data_pred['y_pred'] = predictions\n",
    "\n",
    "# save predicted travel time values\n",
    "data_pred.to_csv(\"predicted_travel_time/NODE.txt\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701c2973-78df-4e4d-b4ad-850e0038544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reidentification algorithm for file:  20230327_0700_1400.txt\n",
      "Running reidentification algorithm for file:  20221206_0945_1200.txt\n",
      "Running reidentification algorithm for file:  20221214_0645_0715.txt\n",
      "Running reidentification algorithm for file:  20230327_1415_1900.txt\n",
      "Running reidentification algorithm for file:  20221206_0845_0915.txt\n",
      "Running reidentification algorithm for file:  20221214_0945_1015.txt\n",
      "\n",
      "Num of candidate pairs: 1040\n",
      "\n",
      "TP, FP, FN: 534, 31, 85\n",
      "Accuracy, Precision, Recall, F1: 0.8885, 0.9451, 0.8627, 0.9020\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tt_thru_min, tt_thru_max = 2.5, 12 # min, max of through travel time to constrain search space\n",
    "\n",
    "# function to process candidate match pairs\n",
    "def reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file):\n",
    "    thru_match_initial = [] # store initial candidate match pairs of adv to stop-bar det\n",
    "    \n",
    "    for i in id_adv:\n",
    "        adv_time = adf[adf.ID == i].TimeStamp.values[0]\n",
    "        adv_lane = adf[adf.ID == i].Lane.values[0]\n",
    "\n",
    "        # stop-bar det IDs on the same lane to look for a match\n",
    "        id_stop_look = set(sdf[sdf.Lane == adv_lane].ID)\n",
    "\n",
    "        for j in id_stop_look:\n",
    "            stop_time = sdf[sdf.ID == j].TimeStamp.values[0]\n",
    "\n",
    "            if stop_time > adv_time: # look forward in timestamp\n",
    "                tt_adv_stop = (stop_time - adv_time) / np.timedelta64(1, 's') # paired travel time\n",
    "\n",
    "                if tt_thru_min <= tt_adv_stop <= tt_thru_max:\n",
    "                    # get predicted travel time for file and id_adv\n",
    "                    Xi = data_pred.copy(deep = True)\n",
    "                    Xi = Xi[(Xi.file == file[:-4]) & (Xi.adv == i)].reset_index(drop = True) # discard .txt\n",
    "                    \n",
    "                    tt_predict = Xi.loc[0, 'y_pred'] # predicted travel time\n",
    "                    tt_diff = round(abs(tt_adv_stop - tt_predict), 4) # abs diff between paired & predicted\n",
    "\n",
    "                    # store adv ID, stop ID, travel time diff\n",
    "                    thru_match_initial.append([i, j, tt_diff])\n",
    "\n",
    "    # dicts to store the lowest error for each adv, stop ID\n",
    "    seen_adv_id, seen_stop_id = {}, {}\n",
    "\n",
    "    # iterate through each candidate pair\n",
    "    for pair in thru_match_initial:\n",
    "        adv_id, stop_id, error = pair\n",
    "\n",
    "        # check if adv ID not seen or if error is lower than seen error for that adv ID\n",
    "        if (adv_id not in seen_adv_id) or (error < seen_adv_id[adv_id][1]):\n",
    "            seen_adv_id[adv_id] = list([stop_id, error])\n",
    "\n",
    "        # check if stop ID not seen or if error is lower than seen error for that stop ID\n",
    "        if (stop_id not in seen_stop_id) or (error < seen_stop_id[stop_id][1]):\n",
    "            seen_stop_id[stop_id] = list([adv_id, error])\n",
    "\n",
    "    # match pairs for adv with lowest error\n",
    "    df_adv = pd.DataFrame(seen_adv_id, index = ['adv', 'stop']).T.reset_index()\n",
    "    df_adv.columns = ['adv', 'stop', 'error']\n",
    "\n",
    "    # match pairs for stop with lowest error\n",
    "    df_stop = pd.DataFrame(seen_stop_id, index = ['stop', 'adv']).T.reset_index()\n",
    "    df_stop.columns = ['stop', 'adv', 'error']\n",
    "    \n",
    "    return {'df_adv': df_adv, 'df_stop': df_stop}\n",
    "\n",
    "file_path = \"data\"\n",
    "files = os.listdir(file_path)  # list of processed files to run through reidentifying algorithm\n",
    "\n",
    "df_result = [] # store reidentified match pairs from each file\n",
    "\n",
    "for file in files:\n",
    "    print(\"Running reidentification algorithm for file: \", file)\n",
    "    # read events-processed file with timestamp data\n",
    "    df = pd.read_csv(os.path.join(file_path, file), sep = '\\t')\n",
    "    df.TimeStamp = pd.to_datetime(df.TimeStamp, format = '%Y-%m-%d %H:%M:%S.%f').sort_values()\n",
    "    df.dropna(axis = 0, inplace = True) # drop rows with Nan\n",
    "\n",
    "    # data frames for adv and stop-bar det\n",
    "    adf = df[df.Det == 'adv']\n",
    "    sdf = df[df.Det == 'stop']\n",
    "    id_adv = list(sorted(adf.ID))\n",
    "\n",
    "    # process candidate match pairs to get datasets of adv and stop pairs\n",
    "    candidate_match_result = reidentifyMatchPairs(adf, sdf, id_adv, data_pred, file)\n",
    "    df_adv = candidate_match_result['df_adv']\n",
    "    df_stop = candidate_match_result['df_stop']\n",
    "\n",
    "    # resulting common match pairs\n",
    "    df_match_pair = df_adv.merge(df_stop, on = ['adv', 'stop', 'error'])\n",
    "    df_match_pair['file'] = file[:-4]\n",
    "    df_result.append(df_match_pair)\n",
    "\n",
    "match_result = pd.concat(df_result)\n",
    "match_result.to_csv(\"reidentification_result/NODE.txt\", sep = '\\t')\n",
    "\n",
    "# ground-truth match pairs for index cols\n",
    "match_ground = data_test.copy(deep = True)\n",
    "num_candidate_pairs = match_ground.shape[0]\n",
    "print(f\"\\nNum of candidate pairs: {num_candidate_pairs}\\n\")\n",
    "\n",
    "# filter ground-truth match pairs for match == 1 and select index cols\n",
    "match_ground = match_ground[match_ground.match == 1][index_cols]\n",
    "\n",
    "# get true positive (TP), false positive (FP), and false negative (FN) matches   \n",
    "match_TP = pd.merge(match_result, match_ground, on = index_cols)\n",
    "match_FP = match_result.merge(match_ground, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "match_FN = match_ground.merge(match_result, on = index_cols, how = 'left', indicator = True).query('_merge == \"left_only\"').drop(columns = '_merge')\n",
    "\n",
    "# num of TP, FP, FN\n",
    "TP, FP, FN = match_TP.shape[0], match_FP.shape[0], match_FN.shape[0]\n",
    "TN = num_candidate_pairs - TP - FP - FN\n",
    "\n",
    "# compute metrics\n",
    "accuracy = round((TP + TN) / (TP + FP + FN + TN), 4)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2*precision*recall / (precision + recall)\n",
    "\n",
    "print(f\"TP, FP, FN: {TP}, {FP}, {FN}\")\n",
    "print(f\"Accuracy, Precision, Recall, F1: {accuracy:.4f}, {precision:.4f}, {recall:.4f}, {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
